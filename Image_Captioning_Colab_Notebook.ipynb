{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Image Captioning with CNN Encoder and Transformer Decoder\n",
                "\n",
                "This notebook implements an image captioning system using:\n",
                "- **CNN Encoder**: Pretrained ResNet50\n",
                "- **Transformer Decoder**: PyTorch's nn.TransformerDecoder\n",
                "- **Dataset**: COCO 2017\n",
                "\n",
                "Optimized for Google Colab with limited GPU memory.\n",
                "\n",
                "**Run cells sequentially from top to bottom.**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Environment Setup\n",
                "\n",
                "Install required libraries and download NLTK data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q pycocotools nltk tqdm\n",
                "\n",
                "import nltk\n",
                "nltk.download('punkt', quiet=True)\n",
                "\n",
                "print(\"‚úÖ Environment setup complete!\")\n",
                "print(\"   - pycocotools installed\")\n",
                "print(\"   - nltk installed\")\n",
                "print(\"   - punkt tokenizer downloaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Dataset Download\n",
                "\n",
                "Download COCO 2017 validation dataset (~1GB)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Create directories\n",
                "!mkdir -p /content/coco/images\n",
                "!mkdir -p /content/coco/annotations\n",
                "\n",
                "# Download validation images (~1GB)\n",
                "print(\"Downloading COCO validation images (this may take a few minutes)...\")\n",
                "!wget -q --show-progress http://images.cocodataset.org/zips/val2017.zip\n",
                "!unzip -q val2017.zip -d /content/coco/images/\n",
                "!rm val2017.zip\n",
                "\n",
                "# Download annotations\n",
                "print(\"\\nDownloading COCO annotations...\")\n",
                "!wget -q --show-progress http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
                "!unzip -q annotations_trainval2017.zip -d /content/coco/\n",
                "!rm annotations_trainval2017.zip\n",
                "\n",
                "# Verify download\n",
                "print(\"\\n‚úÖ Dataset downloaded!\")\n",
                "print(\"\\nüìÅ Directory structure:\")\n",
                "!ls -lh /content/coco/images/val2017 | head -5\n",
                "print(f\"\\nTotal images: {len(os.listdir('/content/coco/images/val2017'))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Imports & Configuration\n",
                "\n",
                "Import libraries and define hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torchvision.models as models\n",
                "import torchvision.transforms as transforms\n",
                "\n",
                "from pycocotools.coco import COCO\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from collections import Counter\n",
                "import math\n",
                "import random\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "random.seed(42)\n",
                "\n",
                "# ============================================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================================\n",
                "\n",
                "# Paths\n",
                "IMAGES_DIR = '/content/coco/images/val2017'\n",
                "ANNOTATIONS_FILE = '/content/coco/annotations/captions_val2017.json'\n",
                "\n",
                "# Dataset limits (for memory optimization)\n",
                "MAX_SAMPLES = 500        # Limit to 500 images for quick training\n",
                "VOCAB_THRESHOLD = 3      # Minimum word frequency\n",
                "MAX_CAPTION_LENGTH = 50  # Maximum caption length\n",
                "\n",
                "# Model hyperparameters\n",
                "EMBED_DIM = 256          # Embedding dimension (small for limited GPU)\n",
                "NUM_HEADS = 8            # Number of attention heads\n",
                "NUM_LAYERS = 3           # Number of transformer layers\n",
                "DIM_FEEDFORWARD = 1024   # Feedforward dimension\n",
                "DROPOUT = 0.1            # Dropout rate\n",
                "\n",
                "# Training hyperparameters\n",
                "BATCH_SIZE = 8           # Batch size (small for limited GPU)\n",
                "NUM_EPOCHS = 5           # Number of epochs\n",
                "LEARNING_RATE = 1e-4     # Learning rate\n",
                "\n",
                "# Image preprocessing\n",
                "IMAGE_SIZE = 224\n",
                "IMAGE_MEAN = [0.485, 0.456, 0.406]  # ImageNet mean\n",
                "IMAGE_STD = [0.229, 0.224, 0.225]   # ImageNet std\n",
                "\n",
                "# Device\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"Configuration\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Device: {DEVICE}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "print(f\"Max samples: {MAX_SAMPLES}\")\n",
                "print(f\"Batch size: {BATCH_SIZE}\")\n",
                "print(f\"Epochs: {NUM_EPOCHS}\")\n",
                "print(f\"Embed dim: {EMBED_DIM}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Vocabulary & Tokenization\n",
                "\n",
                "Build vocabulary from COCO captions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Vocabulary:\n",
                "    \"\"\"Vocabulary for word-to-index mapping.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.word2idx = {}\n",
                "        self.idx2word = {}\n",
                "        self.idx = 0\n",
                "        \n",
                "        # Special tokens\n",
                "        self.pad_token = '<pad>'\n",
                "        self.start_token = '<start>'\n",
                "        self.end_token = '<end>'\n",
                "        self.unk_token = '<unk>'\n",
                "        \n",
                "        # Add special tokens\n",
                "        for token in [self.pad_token, self.start_token, self.end_token, self.unk_token]:\n",
                "            self.add_word(token)\n",
                "    \n",
                "    def add_word(self, word):\n",
                "        if word not in self.word2idx:\n",
                "            self.word2idx[word] = self.idx\n",
                "            self.idx2word[self.idx] = word\n",
                "            self.idx += 1\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.word2idx)\n",
                "    \n",
                "    def __call__(self, word):\n",
                "        return self.word2idx.get(word, self.word2idx[self.unk_token])\n",
                "\n",
                "\n",
                "def build_vocabulary(annotation_file, threshold=3, max_samples=None):\n",
                "    \"\"\"Build vocabulary from COCO annotations.\"\"\"\n",
                "    \n",
                "    print(\"Building vocabulary...\")\n",
                "    coco = COCO(annotation_file)\n",
                "    counter = Counter()\n",
                "    ids = list(coco.anns.keys())\n",
                "    \n",
                "    # Limit samples\n",
                "    if max_samples:\n",
                "        ids = ids[:max_samples * 5]  # Use more captions for vocab\n",
                "    \n",
                "    # Count words\n",
                "    for ann_id in tqdm(ids, desc=\"Tokenizing captions\"):\n",
                "        caption = str(coco.anns[ann_id]['caption']).lower()\n",
                "        tokens = caption.split()\n",
                "        counter.update(tokens)\n",
                "    \n",
                "    # Create vocabulary\n",
                "    vocab = Vocabulary()\n",
                "    for word, count in counter.items():\n",
                "        if count >= threshold:\n",
                "            vocab.add_word(word)\n",
                "    \n",
                "    print(f\"‚úÖ Vocabulary size: {len(vocab)}\")\n",
                "    return vocab\n",
                "\n",
                "\n",
                "# Build vocabulary\n",
                "vocab = build_vocabulary(ANNOTATIONS_FILE, VOCAB_THRESHOLD, MAX_SAMPLES)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 5: COCO Dataset & DataLoader\n",
                "\n",
                "Implement PyTorch Dataset and DataLoader for COCO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class COCODataset(Dataset):\n",
                "    \"\"\"COCO Dataset for image captioning.\"\"\"\n",
                "    \n",
                "    def __init__(self, root, annotation_file, vocab, transform=None, max_samples=None):\n",
                "        self.root = root\n",
                "        self.coco = COCO(annotation_file)\n",
                "        self.vocab = vocab\n",
                "        self.transform = transform\n",
                "        self.ids = list(self.coco.anns.keys())\n",
                "        \n",
                "        # Limit dataset\n",
                "        if max_samples:\n",
                "            self.ids = self.ids[:max_samples]\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.ids)\n",
                "    \n",
                "    def __getitem__(self, index):\n",
                "        ann_id = self.ids[index]\n",
                "        annotation = self.coco.anns[ann_id]\n",
                "        \n",
                "        # Load image\n",
                "        img_id = annotation['image_id']\n",
                "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
                "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
                "        \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        \n",
                "        # Process caption\n",
                "        caption = str(annotation['caption']).lower()\n",
                "        tokens = caption.split()\n",
                "        \n",
                "        # Convert to indices: <start> + tokens + <end>\n",
                "        caption_indices = [self.vocab(self.vocab.start_token)]\n",
                "        caption_indices.extend([self.vocab(token) for token in tokens])\n",
                "        caption_indices.append(self.vocab(self.vocab.end_token))\n",
                "        \n",
                "        # Pad to max length\n",
                "        length = len(caption_indices)\n",
                "        if length < MAX_CAPTION_LENGTH:\n",
                "            caption_indices.extend([self.vocab(self.vocab.pad_token)] * (MAX_CAPTION_LENGTH - length))\n",
                "        else:\n",
                "            caption_indices = caption_indices[:MAX_CAPTION_LENGTH]\n",
                "            length = MAX_CAPTION_LENGTH\n",
                "        \n",
                "        return image, torch.LongTensor(caption_indices), length\n",
                "\n",
                "\n",
                "def collate_fn(batch):\n",
                "    \"\"\"Collate function for DataLoader.\"\"\"\n",
                "    batch.sort(key=lambda x: x[2], reverse=True)\n",
                "    images, captions, lengths = zip(*batch)\n",
                "    images = torch.stack(images, 0)\n",
                "    captions = torch.stack(captions, 0)\n",
                "    return images, captions, list(lengths)\n",
                "\n",
                "\n",
                "# Image transforms\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=IMAGE_MEAN, std=IMAGE_STD)\n",
                "])\n",
                "\n",
                "# Create dataset and dataloader\n",
                "dataset = COCODataset(IMAGES_DIR, ANNOTATIONS_FILE, vocab, transform, MAX_SAMPLES)\n",
                "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
                "                       num_workers=2, collate_fn=collate_fn)\n",
                "\n",
                "print(f\"‚úÖ Dataset created: {len(dataset)} samples\")\n",
                "print(f\"‚úÖ DataLoader created: {len(dataloader)} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 6: CNN Encoder\n",
                "\n",
                "CNN Encoder using pretrained ResNet50"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Encoder(nn.Module):\n",
                "    \"\"\"ResNet50 CNN Encoder.\"\"\"\n",
                "    \n",
                "    def __init__(self, embed_dim=256):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Load pretrained ResNet50\n",
                "        resnet = models.resnet50(pretrained=True)\n",
                "        \n",
                "        # Remove final layers\n",
                "        modules = list(resnet.children())[:-2]\n",
                "        self.resnet = nn.Sequential(*modules)\n",
                "        \n",
                "        # Freeze ResNet weights\n",
                "        for param in self.resnet.parameters():\n",
                "            param.requires_grad = False\n",
                "        \n",
                "        # Adaptive pooling and projection\n",
                "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
                "        self.projection = nn.Linear(2048, embed_dim)\n",
                "    \n",
                "    def forward(self, images):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            images: (batch, 3, 224, 224)\n",
                "        Returns:\n",
                "            features: (batch, 49, embed_dim)\n",
                "        \"\"\"\n",
                "        # Extract features: (batch, 2048, 7, 7)\n",
                "        features = self.resnet(images)\n",
                "        features = self.adaptive_pool(features)\n",
                "        \n",
                "        # Reshape: (batch, 49, 2048)\n",
                "        batch_size = features.size(0)\n",
                "        features = features.permute(0, 2, 3, 1).contiguous()\n",
                "        features = features.view(batch_size, -1, 2048)\n",
                "        \n",
                "        # Project: (batch, 49, embed_dim)\n",
                "        features = self.projection(features)\n",
                "        \n",
                "        return features\n",
                "\n",
                "\n",
                "# Test encoder\n",
                "encoder = Encoder(EMBED_DIM).to(DEVICE)\n",
                "test_images = torch.randn(2, 3, 224, 224).to(DEVICE)\n",
                "test_features = encoder(test_images)\n",
                "\n",
                "print(f\"‚úÖ Encoder created\")\n",
                "print(f\"   Input shape: {test_images.shape}\")\n",
                "print(f\"   Output shape: {test_features.shape}\")\n",
                "print(f\"   Expected: (2, 49, {EMBED_DIM})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 7: Transformer Decoder\n",
                "\n",
                "Transformer Decoder with positional encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
                "    \n",
                "    def __init__(self, embed_dim, max_len=5000, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        \n",
                "        # Create positional encoding\n",
                "        pe = torch.zeros(max_len, embed_dim)\n",
                "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
                "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * \n",
                "                            (-math.log(10000.0) / embed_dim))\n",
                "        \n",
                "        pe[:, 0::2] = torch.sin(position * div_term)\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)\n",
                "        pe = pe.unsqueeze(0)\n",
                "        \n",
                "        self.register_buffer('pe', pe)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = x + self.pe[:, :x.size(1), :]\n",
                "        return self.dropout(x)\n",
                "\n",
                "\n",
                "class Decoder(nn.Module):\n",
                "    \"\"\"Transformer Decoder.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size, embed_dim=256, num_heads=8, \n",
                "                 num_layers=3, dim_feedforward=1024, dropout=0.1):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.embed_dim = embed_dim\n",
                "        self.vocab_size = vocab_size\n",
                "        \n",
                "        # Embedding and positional encoding\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
                "        self.pos_encoder = PositionalEncoding(embed_dim, dropout=dropout)\n",
                "        \n",
                "        # Transformer decoder\n",
                "        decoder_layer = nn.TransformerDecoderLayer(\n",
                "            d_model=embed_dim,\n",
                "            nhead=num_heads,\n",
                "            dim_feedforward=dim_feedforward,\n",
                "            dropout=dropout,\n",
                "            batch_first=True\n",
                "        )\n",
                "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
                "        \n",
                "        # Output projection\n",
                "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
                "        \n",
                "        # Initialize weights\n",
                "        nn.init.xavier_uniform_(self.embedding.weight)\n",
                "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
                "        nn.init.zeros_(self.fc_out.bias)\n",
                "    \n",
                "    def forward(self, captions, encoder_out):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            captions: (batch, seq_len)\n",
                "            encoder_out: (batch, 49, embed_dim)\n",
                "        Returns:\n",
                "            predictions: (batch, seq_len, vocab_size)\n",
                "        \"\"\"\n",
                "        # Embed and add positional encoding\n",
                "        embeddings = self.embedding(captions)\n",
                "        embeddings = self.pos_encoder(embeddings)\n",
                "        \n",
                "        # Create causal mask\n",
                "        seq_len = captions.size(1)\n",
                "        tgt_mask = self._generate_causal_mask(seq_len).to(captions.device)\n",
                "        \n",
                "        # Transformer decoder\n",
                "        decoder_out = self.transformer_decoder(\n",
                "            tgt=embeddings,\n",
                "            memory=encoder_out,\n",
                "            tgt_mask=tgt_mask\n",
                "        )\n",
                "        \n",
                "        # Project to vocabulary\n",
                "        predictions = self.fc_out(decoder_out)\n",
                "        \n",
                "        return predictions\n",
                "    \n",
                "    def _generate_causal_mask(self, sz):\n",
                "        \"\"\"Generate causal mask to prevent attending to future tokens.\"\"\"\n",
                "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
                "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
                "        return mask\n",
                "\n",
                "\n",
                "# Test decoder\n",
                "decoder = Decoder(len(vocab), EMBED_DIM, NUM_HEADS, NUM_LAYERS, \n",
                "                 DIM_FEEDFORWARD, DROPOUT).to(DEVICE)\n",
                "test_captions = torch.randint(0, len(vocab), (2, 20)).to(DEVICE)\n",
                "test_predictions = decoder(test_captions, test_features)\n",
                "\n",
                "print(f\"‚úÖ Decoder created\")\n",
                "print(f\"   Input captions shape: {test_captions.shape}\")\n",
                "print(f\"   Input features shape: {test_features.shape}\")\n",
                "print(f\"   Output predictions shape: {test_predictions.shape}\")\n",
                "print(f\"   Expected: (2, 20, {len(vocab)})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 8: Training\n",
                "\n",
                "Training loop with teacher forcing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImageCaptioningModel(nn.Module):\n",
                "    \"\"\"Complete image captioning model.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size, embed_dim=256, num_heads=8, \n",
                "                 num_layers=3, dim_feedforward=1024, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.encoder = Encoder(embed_dim)\n",
                "        self.decoder = Decoder(vocab_size, embed_dim, num_heads, \n",
                "                              num_layers, dim_feedforward, dropout)\n",
                "    \n",
                "    def forward(self, images, captions):\n",
                "        encoder_out = self.encoder(images)\n",
                "        predictions = self.decoder(captions, encoder_out)\n",
                "        return predictions\n",
                "\n",
                "\n",
                "# Create model\n",
                "model = ImageCaptioningModel(len(vocab), EMBED_DIM, NUM_HEADS, \n",
                "                            NUM_LAYERS, DIM_FEEDFORWARD, DROPOUT).to(DEVICE)\n",
                "\n",
                "# Loss and optimizer\n",
                "pad_idx = vocab.word2idx[vocab.pad_token]\n",
                "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "\n",
                "print(f\"‚úÖ Model created\")\n",
                "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
                "\n",
                "# Training loop\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"Training Started\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "train_losses = []\n",
                "\n",
                "for epoch in range(1, NUM_EPOCHS + 1):\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    \n",
                "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}/{NUM_EPOCHS}')\n",
                "    \n",
                "    for images, captions, lengths in progress_bar:\n",
                "        images = images.to(DEVICE)\n",
                "        captions = captions.to(DEVICE)\n",
                "        \n",
                "        # Teacher forcing: input all except last, target all except first\n",
                "        decoder_input = captions[:, :-1]\n",
                "        targets = captions[:, 1:]\n",
                "        \n",
                "        # Forward pass\n",
                "        predictions = model(images, decoder_input)\n",
                "        \n",
                "        # Calculate loss\n",
                "        batch_size, seq_len, vocab_size = predictions.shape\n",
                "        loss = criterion(predictions.reshape(-1, vocab_size), targets.reshape(-1))\n",
                "        \n",
                "        # Backward pass\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        epoch_loss += loss.item()\n",
                "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    avg_loss = epoch_loss / len(dataloader)\n",
                "    train_losses.append(avg_loss)\n",
                "    \n",
                "    print(f\"Epoch {epoch}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
                "\n",
                "print(\"\\n‚úÖ Training complete!\")\n",
                "\n",
                "# Save model\n",
                "torch.save({\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'vocab': vocab,\n",
                "    'config': {\n",
                "        'embed_dim': EMBED_DIM,\n",
                "        'num_heads': NUM_HEADS,\n",
                "        'num_layers': NUM_LAYERS,\n",
                "        'dim_feedforward': DIM_FEEDFORWARD,\n",
                "        'dropout': DROPOUT\n",
                "    }\n",
                "}, '/content/image_captioning_model.pth')\n",
                "\n",
                "print(\"‚úÖ Model saved to /content/image_captioning_model.pth\")\n",
                "\n",
                "# Plot training loss\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, marker='o')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training Loss')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 9: Evaluation & Caption Generation\n",
                "\n",
                "Generate captions and compute BLEU scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_caption(model, image, vocab, max_length=50, device='cuda'):\n",
                "    \"\"\"Generate caption using greedy decoding.\"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        # Encode image\n",
                "        encoder_out = model.encoder(image)\n",
                "        \n",
                "        # Start with <start> token\n",
                "        caption = [vocab.word2idx[vocab.start_token]]\n",
                "        \n",
                "        for _ in range(max_length):\n",
                "            caption_tensor = torch.LongTensor(caption).unsqueeze(0).to(device)\n",
                "            predictions = model.decoder(caption_tensor, encoder_out)\n",
                "            predicted_idx = predictions[0, -1, :].argmax().item()\n",
                "            caption.append(predicted_idx)\n",
                "            \n",
                "            if predicted_idx == vocab.word2idx[vocab.end_token]:\n",
                "                break\n",
                "        \n",
                "        # Convert to words\n",
                "        caption_words = []\n",
                "        for idx in caption[1:-1]:  # Skip <start> and <end>\n",
                "            word = vocab.idx2word.get(idx, vocab.unk_token)\n",
                "            if word != vocab.pad_token:\n",
                "                caption_words.append(word)\n",
                "    \n",
                "    return caption_words\n",
                "\n",
                "\n",
                "def calculate_bleu(reference, candidate):\n",
                "    \"\"\"Calculate BLEU-4 score.\"\"\"\n",
                "    from collections import Counter\n",
                "    \n",
                "    # Calculate n-gram precisions\n",
                "    precisions = []\n",
                "    for n in range(1, 5):\n",
                "        ref_ngrams = Counter([tuple(reference[i:i+n]) for i in range(len(reference)-n+1)])\n",
                "        cand_ngrams = Counter([tuple(candidate[i:i+n]) for i in range(len(candidate)-n+1)])\n",
                "        \n",
                "        if len(cand_ngrams) == 0:\n",
                "            precisions.append(0)\n",
                "            continue\n",
                "        \n",
                "        clipped = sum(min(cand_ngrams[ng], ref_ngrams[ng]) for ng in cand_ngrams)\n",
                "        total = sum(cand_ngrams.values())\n",
                "        precisions.append(clipped / total if total > 0 else 0)\n",
                "    \n",
                "    # Brevity penalty\n",
                "    bp = math.exp(1 - len(reference) / len(candidate)) if len(candidate) < len(reference) else 1\n",
                "    \n",
                "    # BLEU score\n",
                "    if min(precisions) > 0:\n",
                "        bleu = bp * math.exp(sum(math.log(p) for p in precisions) / 4)\n",
                "    else:\n",
                "        bleu = 0\n",
                "    \n",
                "    return bleu\n",
                "\n",
                "\n",
                "def denormalize_image(image_tensor):\n",
                "    \"\"\"Denormalize image for display.\"\"\"\n",
                "    mean = torch.tensor(IMAGE_MEAN).view(3, 1, 1)\n",
                "    std = torch.tensor(IMAGE_STD).view(3, 1, 1)\n",
                "    img = image_tensor * std + mean\n",
                "    img = img.permute(1, 2, 0).numpy()\n",
                "    return np.clip(img, 0, 1)\n",
                "\n",
                "\n",
                "# Load model\n",
                "checkpoint = torch.load('/content/image_captioning_model.pth')\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "print(\"‚úÖ Model loaded for evaluation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 10: Results\n",
                "\n",
                "Display qualitative results with images, captions, and BLEU scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"EVALUATION RESULTS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Evaluate on 5 samples\n",
                "num_samples = 5\n",
                "bleu_scores = []\n",
                "\n",
                "for i in range(num_samples):\n",
                "    # Get sample\n",
                "    image, caption, length = dataset[i]\n",
                "    \n",
                "    # Generate caption\n",
                "    image_batch = image.unsqueeze(0).to(DEVICE)\n",
                "    generated_words = generate_caption(model, image_batch, vocab, 50, DEVICE)\n",
                "    \n",
                "    # Get ground truth\n",
                "    gt_words = []\n",
                "    for idx in caption.tolist():\n",
                "        word = vocab.idx2word[idx]\n",
                "        if word == vocab.end_token:\n",
                "            break\n",
                "        if word not in [vocab.start_token, vocab.pad_token]:\n",
                "            gt_words.append(word)\n",
                "    \n",
                "    # Calculate BLEU\n",
                "    bleu = calculate_bleu(gt_words, generated_words)\n",
                "    bleu_scores.append(bleu)\n",
                "    \n",
                "    # Display\n",
                "    img = denormalize_image(image)\n",
                "    \n",
                "    plt.figure(figsize=(12, 8))\n",
                "    plt.imshow(img)\n",
                "    plt.axis('off')\n",
                "    \n",
                "    title = f\"Sample {i+1}\\n\\n\"\n",
                "    title += f\"Generated Caption:\\n{' '.join(generated_words)}\\n\\n\"\n",
                "    title += f\"Ground Truth Caption:\\n{' '.join(gt_words)}\\n\\n\"\n",
                "    title += f\"BLEU-4 Score: {bleu:.4f}\"\n",
                "    \n",
                "    plt.title(title, fontsize=14, pad=20, wrap=True)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nSample {i+1}:\")\n",
                "    print(f\"  Generated:    {' '.join(generated_words)}\")\n",
                "    print(f\"  Ground Truth: {' '.join(gt_words)}\")\n",
                "    print(f\"  BLEU-4:       {bleu:.4f}\")\n",
                "    print(\"-\"*70)\n",
                "\n",
                "# Summary statistics\n",
                "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"FINAL RESULTS\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Samples evaluated: {num_samples}\")\n",
                "print(f\"Average BLEU-4 Score: {avg_bleu:.4f}\")\n",
                "print(f\"Min BLEU-4: {min(bleu_scores):.4f}\")\n",
                "print(f\"Max BLEU-4: {max(bleu_scores):.4f}\")\n",
                "\n",
                "# Interpretation\n",
                "print(\"\\nüìä Performance Interpretation:\")\n",
                "if avg_bleu > 0.30:\n",
                "    print(\"   ‚≠ê‚≠ê‚≠ê Excellent - High quality captions!\")\n",
                "elif avg_bleu > 0.20:\n",
                "    print(\"   ‚≠ê‚≠ê Good - Acceptable caption quality\")\n",
                "elif avg_bleu > 0.10:\n",
                "    print(\"   ‚≠ê Fair - Needs more training\")\n",
                "else:\n",
                "    print(\"   Needs improvement - Train longer or with more data\")\n",
                "\n",
                "print(\"\\n‚úÖ Evaluation complete!\")\n",
                "print(\"\\nüí° Tips for improvement:\")\n",
                "print(\"   - Increase MAX_SAMPLES for more training data\")\n",
                "print(\"   - Increase NUM_EPOCHS for longer training\")\n",
                "print(\"   - Use beam search instead of greedy decoding\")\n",
                "print(\"   - Fine-tune the encoder\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
                "print(\"=\"*70)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}