# ğŸš€ Image Captioning - Modular Structure

Clean, modular image captioning system with separate files for each component.

---

## ğŸ“ Project Structure

```
image_captioning/
â”œâ”€â”€ config.py         # Hyperparameters & configuration
â”œâ”€â”€ vocab.py          # Vocabulary builder
â”œâ”€â”€ dataset.py        # COCO dataset & dataloader
â”œâ”€â”€ encoder.py        # ResNet50 encoder
â”œâ”€â”€ decoder.py        # Transformer decoder
â”œâ”€â”€ train.py          # Training loop
â”œâ”€â”€ evaluate.py       # Caption generation & BLEU
â”œâ”€â”€ requirements.txt  # Dependencies
â”œâ”€â”€ checkpoints/      # Model checkpoints
â””â”€â”€ data/             # COCO dataset
    â””â”€â”€ coco/
```

---

## ğŸ¯ File Descriptions

| File | Purpose | Lines |
|------|---------|-------|
| `config.py` | All hyperparameters in one place | ~120 |
| `vocab.py` | Vocabulary class and builder | ~100 |
| `dataset.py` | COCO dataset, transforms, dataloader | ~150 |
| `encoder.py` | ResNet50 CNN encoder | ~80 |
| `decoder.py` | Transformer decoder | ~200 |
| `train.py` | Complete training loop | ~250 |
| `evaluate.py` | Evaluation with BLEU scores | ~200 |

**Total: 8 files, ~1100 lines**

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Download COCO Dataset

```bash
# Download validation set (~1GB)
wget http://images.cocodataset.org/zips/val2017.zip
unzip val2017.zip -d data/coco/images/

# Download annotations
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
unzip annotations_trainval2017.zip -d data/coco/
```

### 3. Train

```bash
python train.py
```

### 4. Evaluate

```bash
python evaluate.py
```

---

## âš™ï¸ Configuration

Edit `config.py` to change hyperparameters:

```python
# Dataset
MAX_TRAIN_SAMPLES = 500    # Limit training samples
VOCAB_THRESHOLD = 3        # Minimum word frequency

# Model
EMBED_DIM = 256            # Embedding dimension
NUM_LAYERS = 3             # Transformer layers

# Training
BATCH_SIZE = 8             # Batch size
NUM_EPOCHS = 5             # Number of epochs
LEARNING_RATE = 1e-4       # Learning rate
```

---

## ğŸ“Š Module Details

### config.py
- All hyperparameters centralized
- Easy to modify settings
- Display configuration function

### vocab.py
- `Vocabulary` class for wordâ†”index mapping
- `build_vocab()` to create vocabulary
- `save_vocab()` and `load_vocab()` functions
- Special tokens: `<PAD>`, `<START>`, `<END>`, `<UNK>`

### dataset.py
- `COCODataset` class for loading images and captions
- `collate_fn()` for batching variable-length captions
- `get_transform()` for image preprocessing
- `get_dataloader()` convenience function

### encoder.py
- `Encoder` class using ResNet50
- Extracts spatial features (7Ã—7 grid)
- Projects to embedding dimension
- Supports fine-tuning

### decoder.py
- `Decoder` class using PyTorch's `TransformerDecoder`
- `PositionalEncoding` for position information
- Causal masking for autoregressive generation
- `generate_caption()` for greedy decoding

### train.py
- `ImageCaptioningModel` combining encoder + decoder
- `train_epoch()` with teacher forcing
- Mixed precision training (FP16)
- Gradient clipping
- Checkpoint saving

### evaluate.py
- `calculate_bleu()` for BLEU-4 scores
- `evaluate()` function for batch evaluation
- `display_sample()` for visualization
- Ground truth comparison

---

## ğŸ“ Key Features

âœ… **Modular Design** - Each component in separate file  
âœ… **Clean Imports** - No circular dependencies  
âœ… **Centralized Config** - All settings in `config.py`  
âœ… **Bug-Free** - All shape mismatches fixed  
âœ… **Well-Documented** - Clear docstrings everywhere  
âœ… **Production-Ready** - Tested end-to-end  

---

## ğŸ“ˆ Expected Performance

### With 500 Images, 5 Epochs

| Metric | Value |
|--------|-------|
| Training time | ~20 min |
| GPU memory | ~6 GB |
| Final loss | ~3.5 |
| BLEU-4 | ~0.15 |

---

## ğŸ”§ Usage Examples

### Train with Custom Settings

```python
# Edit config.py
MAX_TRAIN_SAMPLES = 1000
NUM_EPOCHS = 10
BATCH_SIZE = 16

# Then run
python train.py
```

### Evaluate on More Samples

```python
# Edit config.py
NUM_EVAL_SAMPLES = 20

# Then run
python evaluate.py
```

### Use in Your Code

```python
from vocab import build_vocab, load_vocab
from dataset import get_dataloader
from encoder import Encoder
from decoder import Decoder
from train import ImageCaptioningModel

# Load vocabulary
vocab = load_vocab('checkpoints/vocab.pkl')

# Create model
model = ImageCaptioningModel(vocab_size=len(vocab))

# Load checkpoint
checkpoint = torch.load('checkpoints/best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# Generate caption
caption = model.generate_caption(image, vocab, max_length=50, device='cuda')
```

---

## âœ… Summary

**Modular structure with:**

- âœ… 8 clean, focused files
- âœ… No code duplication
- âœ… Easy to understand and modify
- âœ… Production-ready
- âœ… Optimized for limited GPU

**Ready to train! ğŸš€**
